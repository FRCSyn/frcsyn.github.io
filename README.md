Welcome to **[FRCSyn](https://codalab.lisn.upsaclay.fr/competitions/15485)**, the **Face Recognition Challenge in the Era of Synthetic Data** organized at **WACV 2024**. 

<p align="center"><img src="https://wacv2024.thecvf.com/wp-content/uploads/2023/09/WACV-Logo_2024-1024x243.png" style="width:35%;height:auto;"></p>

<p align="center"><img src="/assets/images/intraclass.jpg" style="width:75%;height:auto;"></p>

To promote and advance the use of synthetic data for face recognition, we organize the **Face Recognition Challenge in the Era of Synthetic Data (FRCSyn)**. This challenge intends to **explore the application of synthetic data to the field of face recognition** in order to find solutions to the current limitations existed in the technology, for example, in terms of **privacy concerns** associated with real data, **bias in demographic groups** (e.g., ethnicity and gender), and **lack of performance in challenging conditions** such as large age gaps between enrolment and testing, pose variations, occlusions etc.

This challenge intends to provide an in-depth analysis of the following research questions:

- What are the limits of face recognition technology trained only with synthetic data?
- Can the use of synthetic data be beneficial to reduce the current limitations existed in face recognition technology?

FRCSyn challenge will analyze **improvements achieved using synthetic data** and the state-of-the-art face recognition technology in **realistic scenarios**, providing valuable contributions to advance the field.

### FRCSyn Challenge: Summary Paper

üìù **The summary paper of the FRCSyn Challenge is available [here](https://arxiv.org/abs/2311.10476).**   

### News

- **30 Nov 2023** <a href="#schedule">Schedule for the Workshop is available</a>  
- **20 Nov 2023** [Summary paper available on arxiv](https://arxiv.org/abs/2311.10476)
- **30 Oct 2023** FRCSyn Challenge ends
- **10 Oct 2023** Deadline extended to 30 October
- **13 Sep 2023** FRCSyn Challenge starts
- **10 Sep 2023** Website is live!

### Schedule
<div id="schedule">
<table>
<thead>
  <tr>
    <th>Time (HST) </th>
    <th>Duration </th>
    <th>Activity </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>8:20 ‚Äì 8:30 </td>
    <td>10 mins </td>
    <td>Introduction </td>
  </tr>
  <tr>
    <td>8:30 ‚Äì 9:15 </td>
    <td>45 min </td>
    <td>Keynote 1: <a href="https://luminohope.org/">Koki Nagano</a> </td>
  </tr>
  <tr>
    <td>9:15 ‚Äì 10:00 </td>
    <td>45 min </td>
    <td>Keynote 2: <a href="https://www.cs.cmu.edu/~ftorre/">Fernando De la Torre</a> </td>
  </tr>
  <tr>
    <td>10:00 ‚Äì 10:15 </td>
    <td>15 min </td>
    <td>1st Break </td>
  </tr>
  <tr>
    <td>10:15 ‚Äì 10:35 </td>
    <td>20 min </td>
    <td>FRCSyn Challenge </td>
  </tr>
  <tr>
    <td>10:35 ‚Äì 10:45 </td>
    <td>10 min </td>
    <td>FRCSyn Challenge: Q&amp;A </td>
  </tr>
  <tr>
    <td>10:45 ‚Äì 11:20 </td>
    <td>35 min </td>
    <td>Top-ranked Teams (5) </td>
  </tr>
  <tr>
    <td>11:20 ‚Äì 11:35 </td>
    <td>15 min </td>
    <td>Notable Teams (3) </td>
  </tr>
  <tr>
    <td>11:35 ‚Äì 11:50 </td>
    <td>15 min </td>
    <td>2nd Break </td>
  </tr>
  <tr>
    <td>11:50 ‚Äì 12:35 </td>
    <td>45 min </td>
    <td>Keynote 3: <a href="https://cvlab.cse.msu.edu/">Xiaoming Liu</a> </td>
  </tr>
  <tr>
    <td>12:35 ‚Äì 12:45 </td>
    <td>10 min </td>
    <td>Closing Notes </td>
  </tr>
</tbody>
</table>
</div>

### Keynote Speakers

<table>
  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://luminohope.org/">Koki NAGANO</a></strong></p>
        <p align="center"><img src="/assets/images/nagano.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Short bio:</strong> Koki Nagano works at the intersection of Computer Graphics, Vision and AI. Currently, he is a Senior Research Scientist at NVIDIA. His research focuses on realistic digital humans synthesis, neural media synthesis and trustworthy visual computing including detection and prevention of visual misinformation in collaboration with DARPA. His PhD thesis focused on the topic of high-fidelity 3D human capture (USC, 2017), and he was supervised by Prof. Paul Debevec at USC ICT. Previously he was a Principal Scientist at Pinscreen Inc. During his PhD, he worked for Weta Digital on Hollywood blockbuster movies and for the Meta Reality Labs in Pittsburgh. He got his Bachelor of Engineering from Tokyo Institute of Technology with focus on Environmental Design.</p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://www.cs.cmu.edu/~ftorre/">Fernando DE LA TORRE</a></strong></p>
        <p align="center"><img src="/assets/images/delatorre.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Title:</strong> Zero-shot/few-shot  learning for model diagnosis and debiasing generative models, and its applications to face analysis</p>
        <p><strong>Abstract:</strong> In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced (i.e., uniformly sampled over all the important attributes), diverse, and perfectly labeled test dataset is typically expensive, time-consuming, and error-prone.  To address this issue, first, I will describe zero-shot model diagnosis, a technique to assess deep learning model failures in visual data. In particular, the method will evaluate the sensitivity of deep learning models to arbitrary visual attributes without the need of a test set.  In the second part of the talk, I will describe IT-GEN, an inclusive text-to-image generative model that generates images based on human-written prompts and ensures the resulting images are uniformly distributed across attributes of interest. Applications related to face recognition will be described.</p>
        <p><strong>Short bio:</strong> Fernando De la Torre received his B.Sc. degree in Telecommunications, as well as his M.Sc. and Ph. D degrees in Electronic Engineering from La Salle School of Engineering at Ramon Llull University, Barcelona, Spain in 1994, 1996, and 2002, respectively. He has been a research faculty member in the Robotics Institute at Carnegie Mellon University since 2005. In 2014 he founded FacioMetrics LLC to license technology for facial image analysis (acquired by Facebook in 2016). His research interests are in the fields of Computer Vision and Machine Learning. In particular, applications to human health, augmented reality, virtual reality, and methods that focus on the data (not the model). He is directing the Human Sensing Laboratory (HSL).</p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://cvlab.cse.msu.edu/">Xiaoming LIU</a></strong></p>
        <p align="center"><img src="/assets/images/liu.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Title:</strong> Biometric Recognition in the Era of AI Generated Content (AIGC)</p>
        <p><strong>Abstract:</strong> In recent years we have witnessed impressive progress on AIGC (Artificial Intelligence Generated Content). AIGC has many applications in our society, as well as benefits diverse computer vision tasks. In the context of biometric recognition, we believe that the AIGC era calls for innovation on both data synthesis and how to leverage the synthetic data. In this talk, I will present a number of efforts that showcases these innovations, including: 1) how to bridge the gap between the training data distribution and test data distribution; 2) how to generate a complete synthetic database to train face recognition models; 3) how to estimate the 3D body shape from an image of clothed human body; and 4) how to manipulate a human body image by changing its body pose, clothing style, background, and identity. </p>
        <p><strong>Short bio:</strong> Dr. Xiaoming Liu is the MSU Foundation Professor, and Anil and Nandita Jain Endowed Professor at the Department of Computer Science and Engineering of Michigan State University (MSU). He is also a visiting scientist at Google Research. He received Ph.D. degree from Carnegie Mellon University in 2004. Before joining MSU in 2012 he was a research scientist at General Electric (GE) Global Research. He works on computer vision, machine learning, and biometrics especially on face related analysis and 3D vision. Since 2012 he helps to develop a strong computer vision area in MSU who is ranked top 15 in US according to the 5-year statistics at csrankings.org. He is an Associate Editor of IEEE Transactions on Pattern Analysis and Machine Intelligence. He has authored more than 200 scientific publications, and has filed 29 U.S. patents. His work has been cited over 20000 times, with an H-index of 74. He is a fellow of EEE and IAPR.</p>
      </div>
    </td>
  </tr>

</table>

### Tasks

**The FRCSyn challenge focuses on the two following challenges** existed in
current face recognition technology:

- **Task 1**: synthetic data for **demographic bias mitigation**.
- **Task 2**: synthetic data for **overall performance improvement** (e.g., age, pose, expression, occlusion, demographic groups, etc.).
  
**Within each task, there are two sub-tasks that propose alternative approaches for
training face recognition technology**: one exclusively with synthetic data and the other with a possible combination of real and synthetic data.

### Synthetic Datasets

In the FRCSyn Challenge, **we will provide participants with our synthetic datasets after registration in the challenge**. They are based on our two recent approaches:

**DCFace**: a novel framework entirely based on Diffusion models, composed of i) a sampling stage for the generation of synthetic identities X<sub>ID</sub>, and ii) a mixing stage for the generation of images X<sub>ID,sty</sub> with the same identities X<sub>ID</sub> from the sampling stage and the style selected from a ‚Äústyle bank‚Äù of images <sub>Xsty</sub>. 

[Reference](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.html) M. Kim, F. Liu, A. Jain and X. Liu, ‚ÄúDCFace: Synthetic Face Generation with Dual Condition Diffusion Model‚Äù, in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023. 

**GANDiffFace**: a novel framework based on GANs and Diffusion models that provides fully-synthetic face recognition datasets with the desired properties of human face realism, controllable demographic distributions, and realistic intra-class variations. **Best Paper Award at AMFG @ ICCV 2023.**

[Reference](https://arxiv.org/abs/2305.19962) P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, D. Lawatsch, F. Domin, M. Schaubert, ‚ÄúGANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations‚Äù, in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*, 2023. 

### Registration

**The platform used in FRCSyn Challenge is CodaLab. Participants need to register to take part in the challenge**. Please, follow the instructions:

1. Fill up [this form](https://docs.google.com/forms/d/e/1FAIpQLSf8U80MRf5gk5c0QbGxF76TshaxmThVymeHWXUocSyXRkSiMA/viewform?usp=pp_url) including your information.
2. Sign up in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/15485) using **the same email** introduced in step 1).
3. Join in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/15485) the [FRCSyn Challenge](https://frcsyn.github.io/). Just click in the "Participate" tab for the registration.
4. We will give you access once we check everything is correct.
5. You will receive an email with all the instructions to kickstart FRCSyn, including links to download datasets, experimental protocol, and an example of submission file.

### Paper

The **best teams** of each sub-task will be invited to **contribute as co-authors in the summary paper of the FRCSyn challenge**. This paper will be **published in the proceedings of the WACV 2024 conference**. In addition, **top performers will be invited to present their methods at the workshop. This presentation can be virtual**.

### Important Dates

- **13 Sep 2023** FRCSyn starts
- **30 Oct 2023** FRCSyn ends
- **2 Nov 2023** Announcement of winning teams
- **19 Nov 2023** Paper submission with results of the challenge
- **8 Jan 2024** FRCSyn Workshop at WACV 2024

### FRCSyn at WACV 2024: Results

To determine the winners of sub-tasks 1.1 and 1.2 we consider Trade-off Accuracy, defined as the difference between the average and standard deviation of accuracy across demographic groups. To determine the winners of sub-tasks 2.1 and 2.2 we consider the average of verification accuracy across datasets.

<p align="center"><strong>Task 1.1 - synthetic data for bias mitigation</strong></p>
<table>
<thead>
  <tr>
    <th rowspan="2">#</th>
    <th rowspan="2">User</th>
    <th rowspan="2">Entries</th>
    <th rowspan="2">Date of Last Entry</th>
    <th rowspan="2">Team Name</th>
    <th rowspan="2">Trade-off Accuracy (AVG - STD) [%] </th>
    <th rowspan="2">AVG Accuracy [%] </th>
    <th rowspan="2">STD Accuracy [%] </th>
    <th rowspan="2">FNMR@ FMR=1% </th>
    <th rowspan="2">Gap to Real [%] </th>
  </tr>
  <tr>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>
    <td>lens</td>
    <td>44</td>
    <td>10/30/23</td>
    <td>LENS</td>
    <td>92.25 (1)</td>
    <td>93.54 (1)</td>
    <td>1.28 (3)</td>
    <td>15.25 (2)</td>
    <td>-0.74 (7)</td>
  </tr>
  <tr>
    <td>2</td>
    <td>anjith2006</td>
    <td>15</td>
    <td>10/30/23</td>
    <td>Idiap</td>
    <td>91.88 (2)</td>
    <td>93.41 (2)</td>
    <td>1.53 (4)</td>
    <td>13.97 (1)</td>
    <td>-3.80 (4)</td>
  </tr>
  <tr>
    <td>3</td>
    <td>bjgbiesseck</td>
    <td>14</td>
    <td>10/30/23</td>
    <td>BOVIFOCR-UFPR</td>
    <td>90.51 (3)</td>
    <td>92.35 (3)</td>
    <td>1.84 (5)</td>
    <td>16.35 (3)</td>
    <td>4.23 (9)</td>
  </tr>
  <tr>
    <td>4</td>
    <td>ckoutlis</td>
    <td>20</td>
    <td>10/27/23</td>
    <td>MeVer Lab</td>
    <td>87.51 (4)</td>
    <td>89.62 (4)</td>
    <td>2.11 (6)</td>
    <td>32.57 (5)</td>
    <td>5.68 (10)</td>
  </tr>
  <tr>
    <td>5</td>
    <td>asanchez</td>
    <td>6</td>
    <td>10/30/23</td>
    <td>Aphi</td>
    <td>82.24 (5)</td>
    <td>86.01 (5)</td>
    <td>3.77 (10)</td>
    <td>23.80 (4)</td>
    <td>0.84 (8)</td>
  </tr>
</tbody>
</table>

<p align="center"><strong>Task 1.2 - mixed data for bias mitigation</strong></p>
<table>
<thead>
  <tr>
    <th rowspan="2">#</th>
    <th rowspan="2">User</th>
    <th rowspan="2">Entries</th>
    <th rowspan="2">Date of Last Entry</th>
    <th rowspan="2">Team Name</th>
    <th rowspan="2">Trade-off Accuracy (AVG - STD) [%] </th>
    <th rowspan="2">AVG Accuracy [%] </th>
    <th rowspan="2">STD Accuracy [%] </th>
    <th rowspan="2">FNMR@ FMR=1% </th>
    <th rowspan="2">Gap to Real [%] </th>
  </tr>
  <tr>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>
    <td>zhaoweisong</td>
    <td>68</td>
    <td>10/30/23</td>
    <td>CBSR</td>
    <td>95.25 (1)</td>
    <td>96.45 (1)</td>
    <td>1.20 (3)</td>
    <td>8.68 (4)</td>
    <td>-2.10 (5)</td>
  </tr>
  <tr>
    <td>2</td>
    <td>lens</td>
    <td>44</td>
    <td>10/30/23</td>
    <td>LENS</td>
    <td>95.24 (2)</td>
    <td>96.35 (2)</td>
    <td>1.11 (1)</td>
    <td>6.35 (2)</td>
    <td>-5.67 (4)</td>
  </tr>
  <tr>
    <td>3</td>
    <td>ckoutlis</td>
    <td>20</td>
    <td>10/27/23</td>
    <td>MeVer Lab</td>
    <td>93.87 (3)</td>
    <td>95.44 (3)</td>
    <td>1.56 (4)</td>
    <td>9.50 (5)</td>
    <td>-0.78 (6)</td>
  </tr>
  <tr>
    <td>4</td>
    <td>bjgbiesseck</td>
    <td>14</td>
    <td>10/30/23</td>
    <td>BOVIFOCR-UFPR</td>
    <td>93.15 (4)</td>
    <td>95.04 (4)</td>
    <td>1.89 (5)</td>
    <td>10.00 (6)</td>
    <td>1.28 (9)</td>
  </tr>
  <tr>
    <td>5</td>
    <td>atzoriandrea</td>
    <td>8</td>
    <td>10/30/23</td>
    <td>UNICA-FRAUNHOFER IGD</td>
    <td>91.03 (5)</td>
    <td>94.06 (5)</td>
    <td>3.03 (6)</td>
    <td>6.85 (3)</td>
    <td>-10.62 (2)</td>
  </tr>
  <tr>
    <td>6</td>
    <td>anjith2006</td>
    <td>15</td>
    <td>10/30/23</td>
    <td>Idiap</td>
    <td>87.22 (6)</td>
    <td>91.54 (6)</td>
    <td>4.32 (8)</td>
    <td>5.50 (1)</td>
    <td>-0.65 (7)</td>
  </tr>
</tbody>
</table>

<p align="center"><strong>Task 2.1 - synthetic data for performance improvement</strong></p>
<table>
<thead>
  <tr>
    <th rowspan="2">#</th>
    <th rowspan="2">User</th>
    <th rowspan="2">Entries</th>
    <th rowspan="2">Date of Last Entry</th>
    <th rowspan="2">Team Name</th>
    <th rowspan="2">AVG Accuracy [%] </th>
    <th rowspan="2">FNMR@ FMR=1% </th>
    <th rowspan="2">Gap to Real [%] </th>
  </tr>
  <tr>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>
    <td>bjgbiesseck</td>
    <td>14</td>
    <td>10/30/23</td>
    <td>BOVIFOCR-UFPR</td>
    <td>90.50 (1)</td>
    <td>20.83 (1)</td>
    <td>2.66 (3)</td>
  </tr>
  <tr>
    <td>2</td>
    <td>lens</td>
    <td>44</td>
    <td>10/30/23</td>
    <td>LENS</td>
    <td>88.18 (2)</td>
    <td>33.25 (3)</td>
    <td>3.75 (5)</td>
  </tr>
  <tr>
    <td>3</td>
    <td>anjith2006</td>
    <td>15</td>
    <td>10/30/23</td>
    <td>Idiap</td>
    <td>86.39 (3)</td>
    <td>30.73 (2)</td>
    <td>6.39 (6)</td>
  </tr>
  <tr>
    <td>4</td>
    <td>nicolo.didomenico</td>
    <td>5</td>
    <td>10/29/23</td>
    <td>BioLab</td>
    <td>83.93 (4)</td>
    <td>49.51 (5)</td>
    <td>6.88 (7)</td>
  </tr>
  <tr>
    <td>5</td>
    <td>ckoutlis</td>
    <td>20</td>
    <td>10/27/23</td>
    <td>MeVer Lab</td>
    <td>83.45 (5)</td>
    <td>50.05 (6)</td>
    <td>3.20 (4)</td>
  </tr>
  <tr>
    <td>6</td>
    <td>asanchez</td>
    <td>6</td>
    <td>10/30/23</td>
    <td>Aphi</td>
    <td>80.53 (6)</td>
    <td>46.09 (4)</td>
    <td>9.12 (8)</td>
  </tr>
</tbody>
</table>

<p align="center"><strong>Task 2.2 - mixed data for performance improvement</strong></p>
<table>
<thead>
  <tr>
    <th rowspan="2">#</th>
    <th rowspan="2">User</th>
    <th rowspan="2">Entries</th>
    <th rowspan="2">Date of Last Entry</th>
    <th rowspan="2">Team Name</th>
    <th rowspan="2">AVG Accuracy [%] </th>
    <th rowspan="2">FNMR@ FMR=1% </th>
    <th rowspan="2">Gap to Real [%] </th>
  </tr>
  <tr>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>
    <td>zhaoweisong</td>
    <td>68</td>
    <td>10/30/23</td>
    <td>CBSR</td>
    <td>94.95 (1)</td>
    <td>10.82 (1)</td>
    <td>-3.69 (3)</td>
  </tr>
  <tr>
    <td>2</td>
    <td>lens</td>
    <td>44</td>
    <td>10/30/23</td>
    <td>LENS</td>
    <td>92.40 (2)</td>
    <td>17.67 (4)</td>
    <td>-1.63 (4)</td>
  </tr>
  <tr>
    <td>3</td>
    <td>anjith2006</td>
    <td>15</td>
    <td>10/30/23</td>
    <td>Idiap</td>
    <td>91.74 (3)</td>
    <td>23.27 (5)</td>
    <td>0.00 (7)</td>
  </tr>
  <tr>
    <td>4</td>
    <td>bjgbiesseck</td>
    <td>14</td>
    <td>10/30/23</td>
    <td>BOVIFOCR-UFPR</td>
    <td>91.34 (4)</td>
    <td>16.51 (2)</td>
    <td>1.77 (8)</td>
  </tr>
  <tr>
    <td>5</td>
    <td>ckoutlis</td>
    <td>20</td>
    <td>10/27/23</td>
    <td>MeVer Lab</td>
    <td>87.60 (5)</td>
    <td>17.10 (3)</td>
    <td>-1.57 (5)</td>
  </tr>
  <tr>
    <td>6</td>
    <td>atzoriandrea</td>
    <td>8</td>
    <td>10/30/23</td>
    <td>UNICA-FRAUNHOFER IGD</td>
    <td>84.86 (6)</td>
    <td>39.35 (6)</td>
    <td>-27.43 (1)</td>
  </tr>
</tbody>
</table>

### FRCSyn at WACV 2024: Top Teams

**CBSR** <br>
Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei<br>
IIE, CAS, China; School of Cyber Security, UCAS, China; MAIS, CASIA, China; School of Artificial Intelligence, UCAS, China; CAIR, HKISI, CAS, China<br>

**LENS**<br>
Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb<br>
LENS, Inc., US<br>

**BOVIFOCR-UFPR**<br>
Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo F√ºhr, David Menotti<br>
Federal University of Parana, Curitiba, PR, Brazil; Federal Institute of Mato Grosso, Pontes e Lacerda, Brazil; unico - idTech, Brazil<br>

**Idiap**<br>
Alexander Unnervik, Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, S√©bastien Marcel<br>
Idiap Research Institute, Switzerland; Ecole Polytechnique F√©d√©rale de Lausanne, Switzerland; Universite de Lausanne, Switzerland<br>

**MeVer**<br>
Ioannis Sarridis, Christos Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou<br>
Centre for Research and Technology Hellas, Greece; Harokopio University of Athens, Greece<br>

**BioLab**<br>
Nicol√≤ Di Domenico, Guido Borghi, Lorenzo Pellegrini<br>
University of Bologna, Cesena Campus, Italy<br>

**Aphi**<br>
Enrique Mas-Candela, √Ångela S√°nchez-P√©rez<br>
Facephi, Spain<br>

**UNICA-FRAUNHOFER IGD**<br>
Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras<br>
University of Cagliari, Italy; Fraunhofer IGD, Germany; TU Darmstadt, Germany<br>

### Videos

The video presentations of the Top Teams are available [here](https://www.youtube.com/playlist?list=PLxaKatnw5XAW49MV4-2dtsDlRiXyRqzYG).

[![CBSR Team](https://img.youtube.com/vi/oaZI0ku72nI&list=PLxaKatnw5XAW49MV4-2dtsDlRiXyRqzYG&index=6/0.jpg)](https://www.youtube.com/watch?v=oaZI0ku72nI&list=PLxaKatnw5XAW49MV4-2dtsDlRiXyRqzYG&index=6)

### Organizers

<table>
  <tr>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Melzi.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=iGAKK84AAAAJ&hl=it&oi=ao">Pietro Melzi</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Kim.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://mckim.dev/">Minchul Kim</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Tolosana.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://rubentolosana.github.io/">Ruben Tolosana</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Rathgeb.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=_itMaUcAAAAJ&hl=it&oi=ao">Christian Rathgeb</a></p>
        <p align="center">Hochschule Darmstadt, Germany</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Vera.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=KYMQ0tsAAAAJ&hl=it&oi=ao">Ruben Vera-Rodriguez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Morales.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://aythami.me/">Aythami Morales</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/liu.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://www.cse.msu.edu/~liuxm/index2.html">Xiaoming Liu</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Fierrez.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="http://biometrics.eps.uam.es/fierrez/index.php">Julian Fierrez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Ortega.png" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=LwiecBYAAAAJ&hl=en">Javier Ortega-Garcia</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>
</table>

### Fundings

<table>
  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/comunidad.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/gobierno.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/european.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/trespass.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

</table>

---
layout: default
title: "2<sup>nd</sup> Edition FRCSyn: Face Recognition Challenge in the Era of Synthetic Data"
description: CVPR 2024, 18th June 2024, IN PERSON
---

<link rel="stylesheet" href="/css/style.css">

Welcome to the **[second edition of the FRCSyn](https://codalab.lisn.upsaclay.fr/competitions/16970)**, the **Face Recognition Challenge in the Era of Synthetic Data** organized at **CVPR 2024**. 

<p align="center"><img src="/assets/images/completeCVPR_long_low.jpg" style="width:100%;height:auto;"></p>

To promote and advance the use of synthetic data for face recognition, we organize **the second edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn)**. This challenge intends to **explore the application of synthetic data to the field of face recognition** in order to find solutions to the current limitations in the technology, for example, in terms of **privacy concerns** associated with real data, **bias in demographic groups** (e.g., ethnicity and gender), and **lack of performance in challenging conditions** such as large age gaps between enrolment and testing, pose variations, occlusions, etc. 

<strong style="color:green">In this second edition, we propose new sub-tasks that allow participants to train face recognition systems using novel face synthesis methods, with/without restrictions.</strong>

This challenge intends to provide an in-depth analysis of the following research questions:

- What are the limits of face recognition technology trained only with synthetic data?
- Can the use of synthetic data be beneficial to reduce the current limitations in face recognition technology?

FRCSyn challenge will analyze **improvements achieved using synthetic data** and the state-of-the-art face recognition technology in **realistic scenarios**, providing valuable contributions to advance the field.

### News

* 1 Apr 2024 FRCSyn Challenge ends
* 16 Jan 2024 FRCSyn Challenge starts
* 15 Jan 2024 Website is live!

### Keynote Speakers

<table>
  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://research.nvidia.com/person/shalini-de-mello">Shalini DE MELLO</a></strong></p>
        <p align="center">NVIDIA Research</p>
        <p align="center"><img src="/assets/images/Shalini.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Title:</strong> In the Era of Synthetic Avatars: From Training to Verification</p>
        <p><strong>Abstract:</strong> We are witnessing a rise in generative AI technologies that make the creation of digital avatars automated and accessible to anyone. While it is clear that such AI technologies will benefit numerous digital human applications, from video conferencing to AR/VR, their successful adoption hinges on their ability to generalize to real-world data. In this talk, I discuss our recent efforts to utilize synthetic data to create AI models for synthesizing photorealistic humans. First, I will share our recent efforts on learning to generate photorealistic 3D faces from a collection of in-the-wild 2D images at their native image resolution. Furthermore, I will show how such photorealistic 3D synthetic data can be used to train another AI model for downstream applications, such as a single-view reconstruction of photorealistic faces. Lastly, I will address the emerging challenge of verifying the authenticity of synthetic avatars in a world where their legitimate use becomes ubiquitous.</p>
        <p><strong>Short bio:</strong> Shalini De Mello is a Director of Research, New Experiences and a Distinguished Research Scientist at NVIDIA, where she leads the AI-Mediated Reality and Interaction Research Group. Previously, she was a researcher in the Learning and Perception Research Group at NVIDIA, from 2013 to 2023. Her research interests are in AI, computer vision and digital humans. Her research focuses on using AI to re-imagine interactions between humans, and between humans and machines. She has co-authored scores of peer-reviewed publications and patents. Her inventions have contributed to several NVIDIA AI products, including DriveIX, Maxine and TAO Toolkit. She received her Doctoral and Master’s degrees in Electrical and Computer Engineering from the University of Texas at Austin.</p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://www.cs.cmu.edu/~ftorre/">Fernando DE LA TORRE</a></strong></p>
        <p align="center">Carnagie Mellon University</p>
        <p align="center"><img src="/assets/images/delatorre.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Title:</strong>To be determined</p>
        <p><strong>Abstract:</strong>To be determined</p>
        <p><strong>Short bio:</strong> Fernando De la Torre received his B.Sc. degree in Telecommunications, as well as his M.Sc. and Ph. D degrees in Electronic Engineering from La Salle School of Engineering at Ramon Llull University, Barcelona, Spain in 1994, 1996, and 2002, respectively. He has been a research faculty member in the Robotics Institute at Carnegie Mellon University since 2005. In 2014 he founded FacioMetrics LLC to license technology for facial image analysis (acquired by Facebook in 2016). His research interests are in the fields of Computer Vision and Machine Learning. In particular, applications to human health, augmented reality, virtual reality, and methods that focus on the data (not the model). He is directing the Human Sensing Laboratory (HSL).</p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="25%">
      <div>
        <p align="center"><strong><a href="https://www.igd.fraunhofer.de/en/institute/employees/naser-damer.html">Naser Damer</a></strong></p>
        <p align="center">Michigan State University</p>
        <p align="center"><img src="/assets/images/naser.jpg" style="width:90%;height:auto;"></p>
      </div>
    </td>
    <td width="75%">
      <div>
        <p><strong>Title:</strong>To be determined</p>
        <p><strong>Abstract:</strong>To be determined</p>
        <p><strong>Short bio:</strong>Dr. Naser Damer is a senior researcher at the competence center Smart Living & Biometric Technologies, Fraunhofer IGD. He received his master of science degree in electrical engineering from the Technische Universität Kaiserslautern (2010) and his PhD in computer science from the Technischen Universität Darmstadt (2018). He is a researcher at Fraunhofer IGD since 2011 performing applied research, scientific consulting, and system evaluation. His main research interests lie in the fields of biometrics, machine learning and information fusion. He published more than 50 scientific papers in these fields. Dr. Damer is a Principal Investigator at the National Research Center for Applied Cybersecurity CRISP in Darmstadt, Germany. He serves as a reviewer for a number of journals and conferences and as an associate editor for the Visual Computer journal. He represents the German Institute for Standardization (DIN) in ISO/IEC SC37 biometrics standardization committee.</p>
      </div>
    </td>
  </tr>

</table>

### Tasks

**FRCSyn focuses on the two following challenges** in current face recognition technology:

- **Task 1**: synthetic data for **demographic bias mitigation**.
- **Task 2**: synthetic data for **overall performance improvement** (e.g., age, pose, expression, occlusion, demographic groups, etc.).
  
<strong style="color:green">**Within each task, we propose in the 2nd Edition of FRCSyn three sub-tasks, considering alternative approaches for training face recognition technology:**</strong>

1. **Exclusively with synthetic data** constrained to **maximum 500,000 face images** (e.g., 10,000 identities and 50 images per identity). Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section.
2. **Exclusively with synthetic data** with **no limit** on the number of face images. Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section.
3. **Combining real and synthetic data constrained to:** 1) for real data, **only the proposed CASIA-WebFace dataset (around 500,000 real face images)**, provided to the participants after registration. 2) for synthetic data **maximum 500,000 synthetic face images (e.g., 10,000 identities and 50 images per identity).** Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section. **Participants are free to combine the allowed real and synthetic data as they prefer.**

### Synthetic Datasets

One of the novelties in this second edition of the FRCSyn Challenge is that **there are no fixed datasets for the training with synthetic data.** We allow participants to use any generative framework to create the synthetic datasets for the three sub-tasks of the challenge (considering the corresponding limitations of each sub-task in terms of the number of face images used for training).

We list some state-of-the-art generative frameworks that could be used as reference:

**DCFace**: a novel framework entirely based on Diffusion models, composed of i) a sampling stage for the generation of synthetic identities X<sub>ID</sub>, and ii) a mixing stage for the generation of images X<sub>ID,sty</sub> with the same identities X<sub>ID</sub> from the sampling stage and the style selected from a “style bank” of images <sub>Xsty</sub>. 

[Reference](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.html) M. Kim, F. Liu, A. Jain and X. Liu, “DCFace: Synthetic Face Generation with Dual Condition Diffusion Model”, in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023. 

**GANDiffFace**: a novel framework based on GANs and Diffusion models that provides fully-synthetic face recognition datasets with the desired properties of human face realism, controllable demographic distributions, and realistic intra-class variations. **Best Paper Award at AMFG @ ICCV 2023.**

[Reference](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Melzi_GANDiffFace_Controllable_Generation_of_Synthetic_Datasets_for_Face_Recognition_with_ICCVW_2023_paper.html) P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, D. Lawatsch, F. Domin, M. Schaubert, “GANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations”, in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*, 2023. 

**DigiFace-1M**: a large-scale synthetic dataset obtained by rendering digital faces with a computer graphics pipeline. Each identity of DigiFace-1M is defined as the unique combination of facial geometry, texture, eye color, and hairstyle, while other parameters (i.e. pose, expression, environment, and camera distance) are varied to render multiple images.

[Reference](https://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html) G. Bae, M. de La Gorce, T. Baltrusaitis, C. Hewitt, D. Chen, J. Valentin, R. Cipolla, and J. Shen, "DigiFace-1M: 1 Million Digital Face Images for Face Recognition", in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2023.

**IDiff-Face**: a novel approach based on conditional latent diffusion models for synthetic identity generation with realistic identity variations for face recognition training.

[Reference](https://openaccess.thecvf.com/content/ICCV2023/html/Boutros_IDiff-Face_Synthetic-based_Face_Recognition_through_Fizzy_Identity-Conditioned_Diffusion_Model_ICCV_2023_paper.html) F. Boutros, J. Henry Grebe, A. Kuijper, N. Damer, "IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Model", in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2023

**ID3PM**: a novel approach that leverages the stochastic nature of the denoising diffusion process to produce high-quality, identity-preserving face images with various backgrounds, lighting, poses, and expressions.

[Reference](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Kansy_Controllable_Inversion_of_Black-Box_Face_Recognition_Models_via_Diffusion_ICCVW_2023_paper.html) M. Kansy, A. Raël, G. Mignone, J. Naruniec, C. Schroers, M. Gross, R. M. Weber, "Controllable Inversion of Black-Box Face Recognition Models via Diffusion", in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*, 2023.

**SFace**: a privacy-friendly synthetically generated face dataset is proposed, based on the training of StyleGAN2-ADA with real datasets, and the setting of identity labels as class labels to create synthetic data 

[Reference](https://ieeexplore.ieee.org/abstract/document/9318547?casa_token=AR1uiCoUJIoAAAAA:ShPHrYl4-EtWlG72pTbiLLNsmQ6GiwWiKCMy4yFh23_BG4-Y0CsUaWcQTMT3eWIzlg6BvaRphQ) F. Boutros, M. Huber, P. Siebke, T. Rieber, and N. Damer, "SFace: Privacy-friendly and accurate face recognition using synthetic data", in *Proceedings of the IEEE International Joint Conference on Biometrics*, 2022.

**SYNFace**: a novel framework that proposes the use of DiscoFaceGAN for the synthesis of face images, a disentangled learning scheme that enables precise control of targeted face properties such as identity, pose, expression, and illumination

[Reference](https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html) H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, D. Tao, "SYNFace: Face Recognition With Synthetic Data", in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021.

### Registration

**The platform used in the 2nd edition of FRCSyn Challenge is [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970). Participants need to register to take part in the challenge.** <strong style="color:green">Rules and important information are in the CodaLab</strong>. Please, follow the instructions:

1. Fill up [this form](https://docs.google.com/forms/d/e/1FAIpQLScpQ3IqHMojyNClNADVKilVZF1oidjLMV-Q2MaGrn7m6enT9A/viewform?vc=0&c=0&w=1&flr=0) including your information. **We allow a maximum of 6 members per team.**
2. Sign up in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970) using **the same email** introduced in step 1).
3. Join in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970) the [CVPR 2024 FRCSyn Challenge](https://frcsyn.github.io/CVPR2024.html). Just click on the "Participate" tab for the registration.
4. We will give you access once we check everything is correct.
5. You will receive an email with all the instructions to kickstart FRCSyn, including links to download datasets, experimental protocol, and an example of a submission file.

### Paper

The **best teams** of each sub-task will be invited to
**contribute as co-authors in the summary paper of the 2nd edition of FRCSyn challenge, similar to what we did in the first edition at WACV 2024 [(Reference)](https://openaccess.thecvf.com/content/WACV2024W/FRCSyn/html/Melzi_FRCSyn_Challenge_at_WACV_2024_Face_Recognition_Challenge_in_the_WACVW_2024_paper.html)**. This paper will be **published in the proceedings of the CVPR 2024 conference**. In addition, **top performers will be invited to present their methods at the workshop.**

### Important Dates
* **16 January 2024:** 2nd edition of the FRCSyn Challenge starts
* **1 April 2024:** 2nd edition of the FRCSyn Challenge ends 
* **3 April 2024:** Announcement of winning teams
* **14 April 2024:** Paper submission with results of the challenge
* **18 June 2024:** 2nd edition of FRCSyn Workshop at CVPR 2024

### FRCSyn at CVPR 2024: Results
<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="8">Sub-Task&nbsp;&nbsp;&nbsp;1.1 (Bias Mitigation): Synthetic Data (Constrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>TO [%]</td>
    <td>AVG [%]</td>
    <td>SD [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>ID R&amp;D</td>
    <td>96.73</td>
    <td>97.55</td>
    <td>0.82</td>
    <td>3.17</td>
    <td>99.57</td>
    <td>-5.31</td>
  </tr>
  <tr>
    <td>2</td>
    <td>ADMIS</td>
    <td>94.30</td>
    <td>95.10</td>
    <td>0.80</td>
    <td>11.38</td>
    <td>98.96</td>
    <td>1.47</td>
  </tr>
  <tr>
    <td>3</td>
    <td>SRCN_AIVL</td>
    <td>94.06</td>
    <td>95.12</td>
    <td>1.07</td>
    <td>10.72</td>
    <td>98.83</td>
    <td>-0.54</td>
  </tr>
  <tr>
    <td>4</td>
    <td>OPDAI</td>
    <td>93.75</td>
    <td>94.92</td>
    <td>1.17</td>
    <td>11.85</td>
    <td>99.51</td>
    <td>1.02</td>
  </tr>
  <tr>
    <td>5</td>
    <td>CTAI</td>
    <td>93.21</td>
    <td>94.74</td>
    <td>1.53</td>
    <td>14.38</td>
    <td>98.33</td>
    <td>-0.63</td>
  </tr>
  <tr>
    <td>6</td>
    <td>K-IBS-DS</td>
    <td>92.91</td>
    <td>94.11</td>
    <td>1.20</td>
    <td>15.03</td>
    <td>98.47</td>
    <td>1.58</td>
  </tr>
</tbody>
</table>

<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="8">Sub-Task 1.2 (Bias Mitigation): Synthetic Data&nbsp;&nbsp;&nbsp;(Unconstrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>TO [%]</td>
    <td>AVG [%]</td>
    <td>SD [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>ID R&amp;D</td>
    <td>96.73</td>
    <td>97.55</td>
    <td>0.82</td>
    <td>3.17</td>
    <td>99.57</td>
    <td>-5.31</td>
  </tr>
  <tr>
    <td>2</td>
    <td>ADMIS</td>
    <td>95.72</td>
    <td>96.50</td>
    <td>0.78</td>
    <td>6.33</td>
    <td>99.51</td>
    <td>-0.56</td>
  </tr>
  <tr>
    <td>3</td>
    <td>OPDAI</td>
    <td>94.12</td>
    <td>95.22</td>
    <td>1.11</td>
    <td>10.78</td>
    <td>98.92</td>
    <td>0.71</td>
  </tr>
  <tr>
    <td>4</td>
    <td>INESC-IGD</td>
    <td>94.05</td>
    <td>95.22</td>
    <td>1.17</td>
    <td>11.03</td>
    <td>98.70</td>
    <td>1.04</td>
  </tr>
  <tr>
    <td>5</td>
    <td>K-IBS-DS</td>
    <td>93.72</td>
    <td>94.88</td>
    <td>1.16</td>
    <td>12.75</td>
    <td>99.66</td>
    <td>0.77</td>
  </tr>
  <tr>
    <td>6</td>
    <td>CTAI</td>
    <td>93.21</td>
    <td>94.74</td>
    <td>1.53</td>
    <td>14.38</td>
    <td>98.33</td>
    <td>-0.63</td>
  </tr>
</tbody>
</table>

<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="8">Sub-Task 1.3 (Bias Mitigation): Synthetic + Real Data&nbsp;&nbsp;&nbsp;(Constrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>TO [%]</td>
    <td>AVG [%]</td>
    <td>SD [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>ADMIS</td>
    <td>96.50</td>
    <td>97.25</td>
    <td>0.75</td>
    <td>3.90</td>
    <td>99.72</td>
    <td>-1.33</td>
  </tr>
  <tr>
    <td>2</td>
    <td>K-IBS-DS</td>
    <td>96.17</td>
    <td>96.92</td>
    <td>0.75</td>
    <td>5.88</td>
    <td>99.54</td>
    <td>-1.37</td>
  </tr>
  <tr>
    <td>3</td>
    <td>UNICA-IGD-LSI</td>
    <td>96.00</td>
    <td>96.70</td>
    <td>0.70</td>
    <td>5.90</td>
    <td>99.49</td>
    <td>-5.33</td>
  </tr>
  <tr>
    <td>4</td>
    <td>OPDAI</td>
    <td>95.96</td>
    <td>96.80</td>
    <td>0.84</td>
    <td>4.90</td>
    <td>99.54</td>
    <td>-0.03</td>
  </tr>
  <tr>
    <td>5</td>
    <td>INESC-IGD</td>
    <td>95.65</td>
    <td>96.33</td>
    <td>0.67</td>
    <td>6.15</td>
    <td>99.18</td>
    <td>-0.12</td>
  </tr>
  <tr>
    <td>6</td>
    <td>CBSR-Samsung</td>
    <td>95.57</td>
    <td>96.54</td>
    <td>0.97</td>
    <td>5.00</td>
    <td>99.41</td>
    <td>-24.43</td>
  </tr>
</tbody>
</table>

<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="6">Sub-Task&nbsp;&nbsp;&nbsp;2.1 (Bias Mitigation): Synthetic Data (Constrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>AVG [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>OPDAI</td>
    <td>91.93</td>
    <td>17.63</td>
    <td>97.30</td>
    <td>3.09</td>
  </tr>
  <tr>
    <td>2</td>
    <td>ID R&amp;D</td>
    <td>91.86</td>
    <td>10.36</td>
    <td>97.48</td>
    <td>2.99</td>
  </tr>
  <tr>
    <td>3</td>
    <td>ADMIS</td>
    <td>91.19</td>
    <td>20.41</td>
    <td>97.04</td>
    <td>2.78</td>
  </tr>
  <tr>
    <td>4</td>
    <td>K-IBS-DS</td>
    <td>91.05</td>
    <td>24.87</td>
    <td>96.09</td>
    <td>2.60</td>
  </tr>
  <tr>
    <td>5</td>
    <td>CTAI</td>
    <td>90.59</td>
    <td>21.88</td>
    <td>96.40</td>
    <td>-1.94</td>
  </tr>
  <tr>
    <td>6</td>
    <td>BOVIFOCR-UFPR</td>
    <td>89.97</td>
    <td>24.04</td>
    <td>96.70</td>
    <td>3.71</td>
  </tr>
</tbody>
</table>

<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="6">Sub-Task 2.2 (Bias Mitigation): Synthetic Data&nbsp;&nbsp;&nbsp;(Unconstrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>AVG [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Idiap-SynthDistill</td>
    <td>93.50</td>
    <td>9.17</td>
    <td>97.17</td>
    <td>-0.05</td>
  </tr>
  <tr>
    <td>2</td>
    <td>ADMIS</td>
    <td>92.92</td>
    <td>14.45</td>
    <td>97.76</td>
    <td>0.21</td>
  </tr>
  <tr>
    <td>3</td>
    <td>OPDAI</td>
    <td>92.04</td>
    <td>16.48</td>
    <td>97.41</td>
    <td>3.00</td>
  </tr>
  <tr>
    <td>4</td>
    <td>ID R&amp;D</td>
    <td>91.86</td>
    <td>10.36</td>
    <td>97.48</td>
    <td>2.99</td>
  </tr>
  <tr>
    <td>5</td>
    <td>K-IBS-DS</td>
    <td>91.61</td>
    <td>22.48</td>
    <td>96.54</td>
    <td>1.96</td>
  </tr>
  <tr>
    <td>6</td>
    <td>CTAI</td>
    <td>90.59</td>
    <td>21.88</td>
    <td>96.40</td>
    <td>-1.94</td>
  </tr>
</tbody>
</table>

<table>
<thead>
  <tr>
    <th style="text-align: center" colspan="6">Sub-Task  2.3 (Bias&nbsp;&nbsp;&nbsp;Mitigation): Synthetic + Real Data (Constrained)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Pos.</td>
    <td>Team</td>
    <td>AVG [%]</td>
    <td>FNMR@FMR=1%</td>
    <td>AUC [%]</td>
    <td>GAP [%]</td>
  </tr>
  <tr>
    <td>1</td>
    <td>K-IBS-DS</td>
    <td>95.42</td>
    <td>9.49</td>
    <td>98.14</td>
    <td>-2.15</td>
  </tr>
  <tr>
    <td>2</td>
    <td>OPDAI</td>
    <td>95.23</td>
    <td>7.54</td>
    <td>98.70</td>
    <td>-0.52</td>
  </tr>
  <tr>
    <td>3</td>
    <td>CTAI</td>
    <td>94.56</td>
    <td>8.85</td>
    <td>98.41</td>
    <td>-6.01</td>
  </tr>
  <tr>
    <td>4</td>
    <td>CBSR-Samsung</td>
    <td>94.20</td>
    <td>8.62</td>
    <td>98.17</td>
    <td>-4.40</td>
  </tr>
  <tr>
    <td>5</td>
    <td>ADMIS</td>
    <td>94.15</td>
    <td>10.99</td>
    <td>98.46</td>
    <td>-1.10</td>
  </tr>
  <tr>
    <td>6</td>
    <td>ID R&amp;D</td>
    <td>94.05</td>
    <td>8.00</td>
    <td>98.16</td>
    <td>0.07</td>
  </tr>
</tbody>
</table>

### Schedule

**Not Available yet**

### Organizers

<table>
  <tr>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Tolosana.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://rubentolosana.github.io/">Ruben Tolosana</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Ivan_deAndres_Foto_crop.jpeg" style="width:75%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.es/citations?user=4ulTK3wAAAAJ&hl=es&oi=ao">Ivan DeAndres-Tame</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Melzi.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=iGAKK84AAAAJ&hl=it&oi=ao">Pietro Melzi</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Vera.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=KYMQ0tsAAAAJ&hl=it&oi=ao">Ruben Vera-Rodriguez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Kim.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://mckim.dev/">Minchul Kim</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Rathgeb.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=_itMaUcAAAAJ&hl=it&oi=ao">Christian Rathgeb</a></p>
        <p align="center">Hochschule Darmstadt, Germany</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/liu.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://www.cse.msu.edu/~liuxm/index2.html">Xiaoming Liu</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Morales.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://aythami.me/">Aythami Morales</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Fierrez.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="http://biometrics.eps.uam.es/fierrez/index.php">Julian Fierrez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>
  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Ortega.png" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=LwiecBYAAAAJ&hl=en">Javier Ortega-Garcia</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>
</table>

### Fundings

<table>
  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/comunidad.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/CatedraENIA.PNG" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/european.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/trespass.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2" width="50%">
      <div>
        <p align="center"><img src="/assets/images/gobierno.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2" width="50%">
      <div>
        <p align="center"><img src="/assets/images/ELLIS_Madrid_short.PNG" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2" width="50%">
      <div>
        <p align="center"><img src="/assets/images/NextGenerationEU.PNG" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

</table>

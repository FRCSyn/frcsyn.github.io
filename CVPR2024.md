---
layout: default
title: "2<sup>nd</sup> Edition FRCSyn: Face Recognition Challenge in the Era of Synthetic Data"
description: CVPR 2024, 17th-18th June 2024, IN PERSON
---
<link rel="stylesheet" href="/css/style.css">

Welcome to the **[second edition of the FRCSyn](https://codalab.lisn.upsaclay.fr/competitions/16970)**, the **Face Recognition Challenge in the Era of Synthetic Data** organized at **CVPR 2024**. 

<p align="center"><img src="/assets/images/completeCVPR_long_low.jpg" style="width:100%;height:auto;"></p>

To promote and advance the use of synthetic data for face recognition, we organize **the second edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn)**. This challenge intends to **explore the application of synthetic data to the field of face recognition** in order to find solutions to the current limitations in the technology, for example, in terms of **privacy concerns** associated with real data, **bias in demographic groups** (e.g., ethnicity and gender), and **lack of performance in challenging conditions** such as large age gaps between enrolment and testing, pose variations, occlusions, etc. 

<strong style="color:green">In this second edition, we propose new sub-tasks that allow participants to train face recognition systems using novel face synthesis methods, with/without restrictions.</strong>

This challenge intends to provide an in-depth analysis of the following research questions:

- What are the limits of face recognition technology trained only with synthetic data?
- Can the use of synthetic data be beneficial to reduce the current limitations in face recognition technology?

FRCSyn challenge will analyze **improvements achieved using synthetic data** and the state-of-the-art face recognition technology in **realistic scenarios**, providing valuable contributions to advance the field.

### News

* 15 Jan 2024 Website is live!

### Tasks

**FRCSyn focuses on the two following challenges** in current face recognition technology:

- **Task 1**: synthetic data for **demographic bias mitigation**.
- **Task 2**: synthetic data for **overall performance improvement** (e.g., age, pose, expression, occlusion, demographic groups, etc.).
  
<strong style="color:green">**Within each task, we propose in the 2nd Edition of FRCSyn three sub-tasks, considering alternative approaches for training face recognition technology:**</strong>

1. **Exclusively with synthetic data** constrained to **maximum 500,000 face images** (e.g., 10,000 identities and 50 images per identity). Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section.
2. **Exclusively with synthetic data** with **no limit** on the number of face images. Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section.
3. **Combining real and synthetic data constrained to:** 1) for real data, **only the proposed CASIA-WebFace dataset (around 500,000 real face images)**, provided to the participants after registration. 2) for synthetic data **maximum 500,000 synthetic face images (e.g., 10,000 identities and 50 images per identity).** Synthetic face images can be generated using any state-of-the-art methods, no restrictions. We provide some suggestions in the following section. **Participants are free to combine the allowed real and synthetic data as they prefer.**

### Synthetic Datasets

One of the novelties in this second edition of the FRCSyn Challenge is that **there are no fixed datasets for the training with synthetic data.** We allow participants to use any generative framework to create the synthetic datasets for the three sub-tasks of the challenge (considering the corresponding limitations of each sub-task in terms of the number of face images used for training).

We list some state-of-the-art generative frameworks that could be used as reference:

**DCFace**: a novel framework entirely based on Diffusion models, composed of i) a sampling stage for the generation of synthetic identities X<sub>ID</sub>, and ii) a mixing stage for the generation of images X<sub>ID,sty</sub> with the same identities X<sub>ID</sub> from the sampling stage and the style selected from a “style bank” of images <sub>Xsty</sub>. 

[Reference](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.html) M. Kim, F. Liu, A. Jain and X. Liu, “DCFace: Synthetic Face Generation with Dual Condition Diffusion Model”, in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023. 

**GANDiffFace**: a novel framework based on GANs and Diffusion models that provides fully-synthetic face recognition datasets with the desired properties of human face realism, controllable demographic distributions, and realistic intra-class variations. **Best Paper Award at AMFG @ ICCV 2023.**

[Reference](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Melzi_GANDiffFace_Controllable_Generation_of_Synthetic_Datasets_for_Face_Recognition_with_ICCVW_2023_paper.html) P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, D. Lawatsch, F. Domin, M. Schaubert, “GANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations”, in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*, 2023. 

**DigiFace-1M**: a large-scale synthetic dataset obtained by rendering digital faces with a computer graphics pipeline. Each identity of DigiFace-1M is defined as the unique combination of facial geometry, texture, eye color, and hairstyle, while other parameters (i.e. pose, expression, environment, and camera distance) are varied to render multiple images.

[Reference](https://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html) G. Bae, M. de La Gorce, T. Baltrusaitis, C. Hewitt, D. Chen, J. Valentin, R. Cipolla, and J. Shen, "DigiFace-1M: 1 Million Digital Face Images for Face Recognition", in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2023.

**IDiff-Face**: a novel approach based on conditional latent diffusion models for synthetic identity generation with realistic identity variations for face recognition training.

[Reference](https://openaccess.thecvf.com/content/ICCV2023/html/Boutros_IDiff-Face_Synthetic-based_Face_Recognition_through_Fizzy_Identity-Conditioned_Diffusion_Model_ICCV_2023_paper.html) F. Boutros, J. Henry Grebe, A. Kuijper, N. Damer, "IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Model", in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2023

**ID3PM**: a novel approach that leverages the stochastic nature of the denoising diffusion process to produce high-quality, identity-preserving face images with various backgrounds, lighting, poses, and expressions.

[Reference](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Kansy_Controllable_Inversion_of_Black-Box_Face_Recognition_Models_via_Diffusion_ICCVW_2023_paper.html) M. Kansy, A. Raël, G. Mignone, J. Naruniec, C. Schroers, M. Gross, R. M. Weber, "Controllable Inversion of Black-Box Face Recognition Models via Diffusion", in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*, 2023.

**SFace**: a privacy-friendly synthetically generated face dataset is proposed, based on the training of StyleGAN2-ADA with real datasets, and the setting of identity labels as class labels to create synthetic data 

[Reference](https://ieeexplore.ieee.org/abstract/document/9318547?casa_token=AR1uiCoUJIoAAAAA:ShPHrYl4-EtWlG72pTbiLLNsmQ6GiwWiKCMy4yFh23_BG4-Y0CsUaWcQTMT3eWIzlg6BvaRphQ) F. Boutros, M. Huber, P. Siebke, T. Rieber, and N. Damer, "SFace: Privacy-friendly and accurate face recognition using synthetic data", in *Proceedings of the IEEE International Joint Conference on Biometrics*, 2022.

**SYNFace**: a novel framework that proposes the use of DiscoFaceGAN for the synthesis of face images, a disentangled learning scheme that enables precise control of targeted face properties such as identity, pose, expression, and illumination

[Reference](https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html) H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, D. Tao, "SYNFace: Face Recognition With Synthetic Data", in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021.

### Registration

**The platform used in the 2nd edition of FRCSyn Challenge is [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970). Participants need to register to take part in the challenge.** <strong style="color:green">Rules and important information are in the CodaLab</strong>. Please, follow the instructions:

1. Fill up [this form](https://docs.google.com/forms/d/e/1FAIpQLScpQ3IqHMojyNClNADVKilVZF1oidjLMV-Q2MaGrn7m6enT9A/viewform?vc=0&c=0&w=1&flr=0) including your information. **We allow a maximum of 6 members per team.**
2. Sign up in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970) using **the same email** introduced in step 1).
3. Join in [CodaLab](https://codalab.lisn.upsaclay.fr/competitions/16970) the [CVPR 2024 FRCSyn Challenge](https://frcsyn.github.io/CVPR2024.html). Just click on the "Participate" tab for the registration.
4. We will give you access once we check everything is correct.
5. You will receive an email with all the instructions to kickstart FRCSyn, including links to download datasets, experimental protocol, and an example of a submission file.

### Paper

The **best teams** of each sub-task will be invited to
**contribute as co-authors in the summary paper of the 2nd edition of FRCSyn challenge, similar to what we did in the first edition at WACV 2024 [(Reference)](https://openaccess.thecvf.com/content/WACV2024W/FRCSyn/html/Melzi_FRCSyn_Challenge_at_WACV_2024_Face_Recognition_Challenge_in_the_WACVW_2024_paper.html)**. This paper will be **published in the proceedings of the CVPR 2024 conference**. In addition, **top performers will be invited to present their methods at the workshop.**

### Important Dates
* **16 January 2024:** 2nd edition of the FRCSyn Challenge starts
* **1 April 2024:** 2nd edition of the FRCSyn Challenge ends 
* **3 April 2024:** Announcement of winning teams
* **14 April 2024:** Paper submission with results of the challenge
* **17/18 June 2024:** 2nd edition of FRCSyn Workshop at CVPR 2024

### Schedule

**Not Available yet**

### Keynote Speakers

**Not Available yet**

### Organizers

<table>
  <tr>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Tolosana.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://rubentolosana.github.io/">Ruben Tolosana</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Melzi.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=iGAKK84AAAAJ&hl=it&oi=ao">Pietro Melzi</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/IvandeAndres.jpg" style="width:85%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.es/citations?user=4ulTK3wAAAAJ&hl=es&oi=ao">Ivan de Andres</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Vera.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=KYMQ0tsAAAAJ&hl=it&oi=ao">Ruben Vera-Rodriguez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td width="33%">
      <div>
        <p align="center"><img src="/assets/images/Kim.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://mckim.dev/">Minchul Kim</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Rathgeb.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=_itMaUcAAAAJ&hl=it&oi=ao">Christian Rathgeb</a></p>
        <p align="center">Hochschule Darmstadt, Germany</p>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/liu.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://www.cse.msu.edu/~liuxm/index2.html">Xiaoming Liu</a></p>
        <p align="center">Michigan State University, US</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Morales.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://aythami.me/">Aythami Morales</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Fierrez.jpg" style="width:70%;height:auto;"></p>
        <p align="center"><a href="http://biometrics.eps.uam.es/fierrez/index.php">Julian Fierrez</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>
  <tr>
    <td>
      <div>
        <p align="center"><img src="/assets/images/Ortega.png" style="width:70%;height:auto;"></p>
        <p align="center"><a href="https://scholar.google.com/citations?user=LwiecBYAAAAJ&hl=en">Javier Ortega-Garcia</a></p>
        <p align="center">Universidad Autonoma de Madrid, Spain</p>
      </div>
    </td>
  </tr>
</table>

### Fundings

<table>
  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/comunidad.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/gobierno.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/european.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/trespass.png" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="50%">
      <div>
        <p align="center"><img src="/assets/images/ELLIS_Madrid_short.PNG" style="width:70%;height:auto;"></p>
      </div>
    </td>
  </tr>

</table>
